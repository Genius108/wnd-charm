---- C-level changes
support using a shared mmap for pixel data
1. pixel data managed by Eigen C++ matrix lib
2. test all feature calculations and merge with trunk
	test reading 8-bit gray, 16-bit gray, 24-bit RGB and 48-bit RGB
	test long features and lon-color features on all 4 file types (8 tests).
3. use shared memory as cache for pixel data
4. implement parallel feature calculation

-- pix_plane:
	plain array of doubles, single X,Y plane of intensities stored in Eigen::MatrixXd.
	row-major order, the rightmost index "varies the fastest" - X axis
	Dimensions other than X,Y not supported.
		remove partial "Z"/depth support
	explicit RGB image support for color features:
		R, G, B go into separate channels (8 bit) if read form file
		Eigen matrix of HSVcolor structs in clr_plane
		im.clr_data

-- shared memory
	the pixel data needs to have managed state (writeable -> read-only)
	shared IM objects need to have names that reflect the series of transforms performed on the pixels
		fstat(), resolve symlinks, dev_id = S_ISREG(fstat.st_mode) ? fstat.st_dev : fstat.st_rdev
		<dev_id>.<fstat.st_ino>.pix: original pixels
		<dev_id>.<fstat.st_ino>.clr: original color pixels
		--transforms, samples, normalization, etc. chained before .pix/.clr:
		-T<transform>: transform
		-S<x>.<y>.<w>.<h>: sample (?)
		-N<mean>.<std>: normalize to specified mean and std. dev. (?)
		-others?  Do we need to share anything other than transforms?
			-share only things needed to compute more features in parallel (= transforms only).
-- testing
	ReadTIFF identical for t1_s01_c05_ij.tif (16 bits/sample, 12-bit data, 113x113)
	Edge Features () identical
	Otsu() identical
	centroid (x,y) identical
	Object Features () [33] differs (Euler): bug in C-chrm (issue 55)
	Gabor textures differ: 	 bug in C-chrm (issue 55)
	CombFirst4Moments are identical
	All feature algorithms on raw image identical
	Fourier identical
	Chebyshev identical
	All features now identical to > 5 sig figs.
	
	

---- In python, mmap of shared memory
memory = posix_ipc.SharedMemory(shmem_name, posix_ipc.O_CREX, size=params["SHM_SIZE"])
file = os.fdopen(memory.fd)
fp = np.memmap(file, dtype='float32', shape=(3,4))

http://stackoverflow.com/questions/7419159/giving-access-to-shared-memory-after-child-processes-have-already-started/7447103#7447103

---- Python, un-named shared array to numpy (pickled, not mmapped)
import multiprocessing
import ctypes
import numpy as np

shared_array_base = multiprocessing.Array(ctypes.c_double, 10*10)
shared_array = np.ctypeslib.as_array(shared_array_base.get_obj())
shared_array = shared_array.reshape(10, 10)

# No copy was made
assert shared_array.base.base is shared_array_base.get_obj()

# Parallel processing
def my_func(i, def_param=shared_array):
shared_array[i,:] = i

if __name__ == '__main__':
pool = multiprocessing.Pool(processes=4)
pool.map(my_func, range(10))

print shared_array


---- Some thoughts on parallel execution strategy
# wrapper around multitasking.Process
class ConcurrentTask:
	def __init__(self, **kwargs):
		self.func = None
		self.args = None
		self.resource = None
		for k, v in kwargs.iteritems():
			setattr(self, k, v)

	def run():
		pass
	def wait():
		pass
	def allocate():
		pass

class Node:
	def __init__(self, parent=None, **kwargs):
		self.parent = parent
		self.dependency = None
		self.subnodes = []
		self.concurrent_tasks = []
		for k, v in kwargs.iteritems():
			setattr(self, k, v)
		if isinstance (self.parent, Node): # not a root node
			self.parent.subnodes.append(self)
	def add_subnode (self, **kwargs):
		return ( Node (self, **kwargs) )
	def add_task (self, **kwargs):
		self.parent.concurrent_tasks.append( ConcurrentTask (**kwargs) )
		return ( self )

	# ExecuteNode()
	# Executes all sub-nodes
	# wait for the dependency
	# thread the dependant nodes
	# thread the concurrentcies
	def ExecuteNode (self):
		wait_task (self.dependency)
		for subnode in self.subnodes:
			# One thread for the subnode dependency
			subnode.dependency.resource = allocate_resource (subnode.dependency.func, subnode.dependency.args)
			subnode.dependency.resource.start()
			# One thread for the subnode itself
			resource = allocate_resource (subnode.ExecuteNode, subnode)
			resource.start()
		for task in node.concurrent_tasks:
			resource = allocate_resource (task.func, task.args)
			resource.start()
